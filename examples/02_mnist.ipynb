{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "39fba1ef-ec4b-38ca-fca0-0bdb31e294f4"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "6efa16c1-f457-45bb-49a3-69e11c7fae18"
   },
   "outputs": [],
   "source": [
    "# standard library\n",
    "import os\n",
    "import gzip\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "# Other\n",
    "%matplotlib inline\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# EURNN\n",
    "import sys; sys.path.append('..')\n",
    "from torch_eunn import EURNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data parameters\n",
    "num_labels = 10 # Number of different types of labels (1-10)\n",
    "width, height = 28, 28 # width / height of the image\n",
    "num_pixels = width*height\n",
    "\n",
    "# pixel permutation idxs\n",
    "perm_idxs = list(range(num_pixels))\n",
    "np.random.RandomState(seed=0).shuffle(perm_idxs)\n",
    "\n",
    "# reverse pixel permutation idxs\n",
    "rev_perm_idxs = [perm_idxs.index(i) for i in range(num_pixels)]\n",
    "\n",
    "# Training Parameters\n",
    "num_steps = 40000 # Number of training steps to run\n",
    "test_size = 10000 # Test data set size\n",
    "valid_size = 10000 # Validation data set size\n",
    "train_size = 60000 - valid_size # Size of the training set\n",
    "batch_size = 100 # Batch size\n",
    "test_batch_size = 1000 # batch size for calculating the validation/test loss\n",
    "\n",
    "# RNN Parameters\n",
    "num_inputs = 1 # input dimension [1=pixel-by-pixel]\n",
    "num_steps_rnn = num_pixels // num_inputs # sequential dimensionality of rnn\n",
    "num_hidden_rnn = 256 # hidden layer dimension\n",
    "capacity_rnn = 2 # capacity of eunn\n",
    "\n",
    "# Optimization parameters\n",
    "learning_rate = 0.0001 # learning rate\n",
    "\n",
    "# select device ('cuda' or 'cpu')\n",
    "device = 'cuda' # highly recommended to take cuda!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f731214e-9fca-f54d-91be-60475207ba64"
   },
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A custom function to fetch the mnist data from its original source. This function downloads the data from [`http://yann.lecun.com`](http://yann.lecun.com) and saves it as `.npy` in the folder named `mnist_data`. The next time this function is called, the `.npy` files are loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_mnist(redownload=False, verbose=True):\n",
    "    ''' Get MNIST data in npy format\n",
    "\n",
    "    Args:\n",
    "        redownload=False (bool): force redownload, even if file already exists\n",
    "    '''\n",
    "    # check if data is already downloaded. If so, do not download again, except\n",
    "    # when explicitly asked to do so\n",
    "    if (os.path.exists('mnist_data/train.npy')\n",
    "        and os.path.exists('mnist_data/test.npy')\n",
    "        and not redownload):\n",
    "        # load files from data folder\n",
    "        return np.load('mnist_data/train.npy'), np.load('mnist_data/test.npy')\n",
    "\n",
    "    # create folders\n",
    "    if not os.path.isdir('mnist_data'):\n",
    "        os.mkdir('mnist_data')\n",
    "\n",
    "    # check if data is already downloaded. If so, do not download again, except\n",
    "    # when explicitly asked to do so\n",
    "    if not (os.path.exists('mnist_data/train_images.gz')\n",
    "        and os.path.exists('mnist_data/train_labels.gz')\n",
    "        and os.path.exists('mnist_data/test_images.gz')\n",
    "        and os.path.exists('mnist_data/test_labels.gz')\n",
    "        and not redownload):\n",
    "        if verbose:\n",
    "            print('downloading mnist data from http://yann.lecun.com/')\n",
    "        # download data\n",
    "        urlretrieve('http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz', 'mnist_data/train_images.gz')\n",
    "        urlretrieve('http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz', 'mnist_data/train_labels.gz')\n",
    "        urlretrieve('http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz', 'mnist_data/test_images.gz')\n",
    "        urlretrieve('http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz', 'mnist_data/test_labels.gz')\n",
    "\n",
    "    # fill numpy arrays:\n",
    "    train = np.empty((60000,785), dtype='uint8')\n",
    "    test = np.empty((10000,785), dtype='uint8')\n",
    "    \n",
    "    if verbose:\n",
    "        print('extracting mnist data... (takes a while)')\n",
    "\n",
    "    for type, npdata in [('train', train),('test', test)]:\n",
    "        # open the files\n",
    "        with gzip.open('mnist_data/%s_images.gz'%type, 'rb') as data,\\\n",
    "             gzip.open('mnist_data/%s_labels.gz'%type, 'rb') as labels:\n",
    "\n",
    "            # skip the first bytes with metadata of the ubyte file:\n",
    "            data.read(16)\n",
    "            labels.read(8)\n",
    "\n",
    "            # read each byte of the gzip file and save it as a uint8 number\n",
    "            # in the numpy array.\n",
    "            for i in range(npdata.shape[0]):\n",
    "                npdata[i,0] = ord(labels.read(1))\n",
    "                for j in range(784): # append the data after the label\n",
    "                    npdata[i, j+1] = ord(data.read(1))\n",
    "\n",
    "    # save numpy arrays\n",
    "    np.save('mnist_data/train.npy', train)\n",
    "    np.save('mnist_data/test.npy', test)\n",
    "    \n",
    "    if verbose:\n",
    "        print('finished conversion.')\n",
    "\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the custom MNIST data fetcher from `fetch_mnist.py`.\n",
    "\n",
    "The image values are specified by an integer between 0 and 255. We convert these pixel values to a float between 0 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In contrast with the Convolutional Neural Networks, we do a pixel-by-pixel recognition of the digit image where the individial pixels are permuted with a fixed permutation defined by `perm_idx`. This fixed permutation is necessary for good performance of the RNN, as otherwise the the end of the pixel stream contains too many zeros for the RNN to retain its internal state. This is a good benchmark task for a recurrent neural network. The performance of this architecture will obviously be worse than for a convnet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "55dd8ffd-6011-49a3-a1fe-c6933c4187b7",
    "_uuid": "840f7b1c60d1a2d5b2222a7c53b2b9d08aac9169",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data, test_data = fetch_mnist()\n",
    "data = np.vstack([train_data, test_data])\n",
    "np.random.shuffle(data)\n",
    "\n",
    "train_data = data[:-test_size-valid_size]\n",
    "valid_data = data[-test_size-valid_size:-test_size]\n",
    "test_data  = data[-test_size:]\n",
    "\n",
    "def get_values_labels(data):\n",
    "    labels = torch.tensor(data[:,0], dtype=torch.int64, device=device)\n",
    "    values = torch.tensor(data[:,1:][:,perm_idxs]/255, dtype=torch.float32, device=device).view(-1, num_steps_rnn, num_inputs)\n",
    "    return values, labels\n",
    "    \n",
    "train_values, train_labels = get_values_labels(train_data)\n",
    "valid_values, valid_labels = get_values_labels(valid_data)\n",
    "test_values, test_labels = get_values_labels(test_data)\n",
    "\n",
    "print(f'train data shape:\\t{train_values.shape}')\n",
    "print(f'train labels shape:\\t{train_labels.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize the different digits by writing a visualization function that reshapes the 784D train and test values into a 28x28 grid:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_digit(digit_array):\n",
    "    plt.imshow(digit_array.cpu().numpy().reshape(num_pixels)[rev_perm_idxs].reshape(width, height), cmap='Greys')\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.show()\n",
    "    \n",
    "show_digit(train_values[31])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now build a network with two convolutional layers, followed by two fully connected layers. We use the `torch.nn.Module` to create the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create RNN Model\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, num_inputs, num_hidden_rnn, capacity_rnn, num_labels):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        # RNN\n",
    "        self.rnn = EURNN(num_inputs, num_hidden_rnn, capacity=capacity_rnn, batch_first=True)\n",
    "        \n",
    "        # Readout layer\n",
    "        self.output_layer = torch.nn.Linear(num_hidden_rnn, num_labels, bias=True)\n",
    "        \n",
    "        # initialize weights\n",
    "        v = np.sqrt(2)/np.sqrt(num_inputs+num_labels)\n",
    "        self.rnn.input_layer.bias.data[:] = 0.01\n",
    "        self.rnn.modrelu.bias.data[:] = 0.01\n",
    "        self.output_layer.bias.data[:] = 0.01\n",
    "        torch.nn.init.uniform_(self.rnn.input_layer.weight.data, -v, v)\n",
    "        torch.nn.init.uniform_(self.output_layer.weight.data, -v, v)\n",
    "        \n",
    "        # move to device\n",
    "        self.to(device)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out, _ = self.rnn(x)\n",
    "        out = self.output_layer(out[:, -1, :]) \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e33cb0c7-51ed-02ae-ebb2-dcda12832da6"
   },
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(num_inputs, num_hidden_rnn, capacity_rnn, num_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the categorical cross entropy loss for training the model.\n",
    "\n",
    "As optimizer we could use a Gradient Descent optimizer [with or without decaying learning rate] or one of the more sophisticated (and easier to optimize) optimizers like Adam or RMSProp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "7fbe419e-7ce2-4d72-bb31-8b27e8161f1b",
    "_uuid": "bb1b6d4fb5504400ed7678d8e95d0a4478b5f409",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# loss\n",
    "lossfunc = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# accuracy\n",
    "def accuracy(logits, labels):\n",
    "    return 100*np.mean(np.argmax(logits.data.cpu().numpy(), 1) == labels.data.cpu().numpy())\n",
    "\n",
    "# RMSprop Optimizer\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "32786a5c-0388-412d-b6da-ee5ace604eda",
    "_uuid": "9c935ac4a1d1964b85513da422ebf60085dca0e3",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "history = []\n",
    "\n",
    "for step in range(num_steps):\n",
    "    # reset gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # calculate validation accuracy and loss\n",
    "    with torch.no_grad():\n",
    "        if step%(train_size//batch_size) == 0 or step == num_steps - 1:\n",
    "            val_acc = np.zeros(valid_size//test_batch_size)\n",
    "            val_loss = np.zeros(valid_size//test_batch_size)\n",
    "            # we need to split the calculation of the validation loss in batches\n",
    "            # to avoid memory problems.\n",
    "            for i in range(0, valid_size, test_batch_size):\n",
    "                valid_logits = model(valid_values[i:i+test_batch_size])\n",
    "                val_loss[i//test_batch_size] = lossfunc(valid_logits, valid_labels[i:i+test_batch_size]).item()\n",
    "                val_acc[i//test_batch_size] = accuracy(valid_logits, valid_labels[i:i+test_batch_size]).item()\n",
    "            history.append((step, val_loss.mean(), val_acc.mean()))\n",
    "            print(f'Step {step:5.0f}\\t Valid. Acc. = {val_acc.mean():5.2f}')\n",
    "\n",
    "    # train\n",
    "    idxs = np.random.randint(0, train_size, batch_size)\n",
    "    batch_values = train_values[idxs]\n",
    "    batch_labels = train_labels[idxs]\n",
    "    logits = model(batch_values)\n",
    "    loss = lossfunc(logits, batch_labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training history:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps, loss, acc = zip(*history)\n",
    "\n",
    "fig, ax_loss = plt.subplots()\n",
    "ax_acc = ax_loss.twinx()\n",
    "\n",
    "plt.sca(ax_acc)\n",
    "plt.plot(steps, acc, '-o', color='C1')\n",
    "plt.ylabel('Accuracy [%]', color='C1');\n",
    "plt.tick_params('y', colors='C1')\n",
    "m = (min(acc)-1)//10*10; plt.ylim(m,100)\n",
    "plt.yticks([m,(m+100)//2,100])\n",
    "\n",
    "plt.sca(ax_loss)\n",
    "plt.plot(steps, loss, '-o', color='C0')\n",
    "plt.ylabel('Log Loss', color='C0');\n",
    "plt.tick_params('y', colors='C0')\n",
    "m = 1.1*max(loss)\n",
    "plt.ylim(0.01, m)\n",
    "\n",
    "plt.xlim(0, (max(steps)+100)//100*100)\n",
    "plt.xlabel('Training Steps')\n",
    "plt.title('Validation Loss / Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the accuracy on the test set can be evaluated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    # we need to split the calculation of the test loss in batches\n",
    "    # to avoid memory problems.\n",
    "    test_accuracy = np.zeros(test_size//test_batch_size)\n",
    "    for i in range(0, test_size, test_batch_size):\n",
    "        test_logits = model(test_values[i:i+test_batch_size])\n",
    "        test_accuracy[i//test_batch_size] = accuracy(test_logits, test_labels[i:i+test_batch_size]).item()\n",
    "test_accuracy = test_accuracy.mean()\n",
    "print(f'Test Accuracy = {test_accuracy:5.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 315\n",
    "show_digit(test_values[N])\n",
    "test_logits = model(test_values[N:N+1])\n",
    "prediction = torch.argmax(test_logits[0]).item()\n",
    "target = test_labels[N].item()\n",
    "print(f'prediction={prediction}\\ttarget={target}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
